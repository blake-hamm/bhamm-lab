global:
  namespace: models
  image:
    repository: kyuz0/amd-strix-halo-toolboxes
    tag: vulkan-radv
    pullPolicy: IfNotPresent
  pvc:
    storageClassName: local-path
    storage: "10Gi"
  service:
    port: 8080
  zeroscaling:
    enabled: true
    minReplicas: 1
    cooldownPeriod: 1800
    trigger:
      query: >-
        sum(llamacpp:requests_processing{container="{{ .name }}"}) or vector(0)
      threshold: "1"
  resources:
    limits:
      amd.com/gpu: 1
      memory: "50Gi"
      cpu: "4"
    requests:
      amd.com/gpu: 1
      cpu: "1"
      memory: "30Gi"
  tolerations:
    - key: "amd.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  nodeSelector: {}
  litellm:
    manage_config: true
    config:
      litellm_settings:
        timeout: 1800
        stream_timeout: 1800
        request_timeout: 1800
        set_verbose: true
        cache: False
        cache_params:
          type: qdrant-semantic
          qdrant_semantic_cache_embedding_model: qwen-embed
          qdrant_collection_name: litellm
          similarity_threshold: 0.6
      general_settings:
        max_parallel_requests: 3
        global_max_parallel_requests: 3
        store_model_in_db: true
        store_prompts_in_spend_logs: true
      router_settings:
        debug_level: "DEBUG"
        enable_pre_call_check: false
        cooldown_time: 600
        disable_cooldowns: False
        allowed_fails: 999
        default_max_parallel_requests: 3
        timeout: 1800
        stream_timeout: 1800
        num_retries: 0
        retry_after: 999
        retry_policy:
          TimeoutErrorRetries: 0
          AuthenticationErrorRetries: 0
          RateLimitErrorRetries: 0
          ContentPolicyViolationErrorRetries: 0
          InternalServerErrorRetries: 0
        allowed_fails_policy:
          BadRequestErrorAllowedFails: 999
          AuthenticationErrorAllowedFails: 999
          TimeoutErrorAllowedFails: 999
          RateLimitErrorAllowedFails: 999
          ContentPolicyViolationErrorAllowedFails: 999
          InternalServerErrorAllowedFails: 999
        routing_strategy: simple-shuffle
        redis_host: os.environ/REDIS_HOST
        redis_port: os.environ/REDIS_PORT
        redis_password: os.environ/REDIS_PASSWORD
models: []
# - name: example-model
#   enabled: true
#   description: Example model with template
#   pvc:
#     storage: "20Gi"
#     storageClassName: local-path
#   args:
#     - llama-server
#     - -hf
#     - example/model
#     - --host
#     - 0.0.0.0
#     - --metrics
#     - --no-webui
#   template: |
#     {%- if tools %}
#         {{- '<|im_start|>system\n' }}
#         {%- if messages[0]['role'] == 'system' %}
#             {{- messages[0]['content'] }}
#         {%- else %}
#             {{- 'You are a helpful assistant.' }}
#         {%- endif %}
#         {{- "\n\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>" }}
#         {%- for tool in tools %}
#             {{- "\n" }}
#             {{- tool | tojson }}
#         {%- endfor %}
#         {{- "\n</tools>\n\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\n<tool_call>\n{\"name\": <function-name>, \"arguments\": <args-json-object>}\n</tool_call>\n" }}
#     {%- else %}
#         {%- if messages[0]['role'] == 'system' %}
#             {{- '<|im_start|>system\n' + messages[0]['content'] + '<|im_end|>\n' }}
#         {%- else %}
#             {{- '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}
#         {%- endif %}
#     {%- endif %}
#     {%- for message in messages %}
#         {%- if (message.role == "user") or (message.role == "system" and not loop.first) or (message.role == "assistant" and not message.tool_calls) %}
#             {{- '<|im_start|>' + message.role + '\n' + message.content + '<|im_end|>\n' }}
#         {%- endif %}
#     {%- endfor %}
#     {%- if messages[-1]['role'] == "user" %}
#         {{- '<|im_start|>assistant\n' }}
#     {%- endif %}
# - name: gpt-oss-120b
#   enabled: true
#   image:
#     repository: kyuz0/amd-strix-halo-toolboxes
#     tag: vulkan-amdvlk
#   pvc:
#     storage: "100Gi"
#     storageClassName: local-path
#   args:
#     - llama-server
#     - --no-mmap
#     - -ngl
#     - "999"
#     - -fa
#     - "on"
#     - --host
#     - 0.0.0.0
#     - --metrics
#     - --no-webui
#     - --jinja
#     - -ub
#     - "2048"
#     - -b
#     - "2048"
#     - -c
#     - "0"
#     - -v
#     - -hf
#     - ggml-org/gpt-oss-120b-GGUF
#   service:
#     port: 8080
#   resources:
#     limits:
#       amd.com/gpu: 1
#       memory: "100Gi"
#       cpu: "8"
#     requests:
#       amd.com/gpu: 1
#       cpu: "2"
#       memory: "70Gi"
#   zeroscaling:
#     enabled: true
#     minReplicas: 1
#     cooldownPeriod: 1800
#     trigger:
#       query: max(llamacpp:requests_processing{app="gpt-oss-120b"}) or vector(0)
#       threshold: "1"

# - name: another-model
#   enabled: true
#   image:
#     repository: another/repo
#     tag: latest
#   pvc:
#     name: another-model-cache
#     storage: "50Gi"
#     storageClassName: local-path
#   args:
#     - some-other-command
#     - --model-path
#     - /models/another-model
#   service:
#     port: 8081
#   zeroscaling:
#     enabled: false
