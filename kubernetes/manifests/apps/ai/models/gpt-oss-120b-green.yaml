apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gpt-oss-120b-cache
  namespace: models
  annotations:
    k8up.io/backup: "true"
    argocd.argoproj.io/sync-wave: "22"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi
  storageClassName: local-path
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gpt-oss-120b
  namespace: models
  annotations:
    argocd.argoproj.io/sync-wave: "22"
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: gpt-oss-120b
  template:
    metadata:
      labels:
        app: gpt-oss-120b
    spec:
      tolerations:
        - key: "amd.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: gpt-oss-120b
          image: kyuz0/amd-strix-halo-toolboxes:vulkan-amdvlk
          env:
            - name: LLAMA_CACHE
              value: "/models/cache"
            - name: HF_HOME
              value: "/models/cache/huggingface"
          args:
            - llama-server
            - --no-mmap
            - -ngl
            - "999"
            - -fa
            - "on"
            - --host
            - 0.0.0.0
            - --metrics
            - --no-webui
            - --jinja
            - -ub
            - "2048"
            - -b
            - "2048"
            - -c
            - "0"
            - -v
            - -hf
            - ggml-org/gpt-oss-120b-GGUF
          ports:
            - containerPort: 8080
              name: http
          resources:
            limits:
              amd.com/gpu: 1
              memory: 100Gi
              cpu: 8
            requests:
              amd.com/gpu: 1
              cpu: 2
              memory: 70Gi
          volumeMounts:
            - name: model-cache
              mountPath: /models/cache
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 20
            timeoutSeconds: 5
            failureThreshold: 60
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 2
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 0
            periodSeconds: 60
            timeoutSeconds: 5
            failureThreshold: 3
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: gpt-oss-120b-cache
---
apiVersion: v1
kind: Service
metadata:
  name: gpt-oss-120b
  namespace: models
  annotations:
    argocd.argoproj.io/sync-wave: "23"
  labels:
    app: gpt-oss-120b
spec:
  type: ClusterIP
  selector:
    app: gpt-oss-120b
  ports:
    - name: http
      protocol: TCP
      port: 8080
      targetPort: http
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: gpt-oss-120b
  namespace: models
  annotations:
    argocd.argoproj.io/sync-wave: "24"
spec:
  selector:
    matchLabels:
      app: gpt-oss-120b
  endpoints:
    - port: http
      path: /metrics
      interval: 30s
      scrapeTimeout: 10s
      scheme: http
  targetLabels:
    - app
---
apiVersion: elasti.truefoundry.com/v1alpha1
kind: ElastiService
metadata:
  name: gpt-oss-120b
  namespace: models
  annotations:
    argocd.argoproj.io/sync-wave: "25"
spec:
  minTargetReplicas: 1
  service: gpt-oss-120b
  cooldownPeriod: 300
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gpt-oss-120b
  triggers:
    - type: prometheus
      metadata:
        query: max(max_over_time(llamacpp:requests_processing{app="gpt-oss-120b"}[30m])) or vector(0)
        serverAddress: http://monitor-prometheus.monitor.svc.cluster.local:9090
        threshold: "0"
