apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-models
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "30"
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: models
  source:
    repoURL: https://github.com/blake-hamm/bhamm-lab.git
    targetRevision: feature/enhance-openwebui
    path: kubernetes/charts/llm-models
    helm:
      valuesObject:
        models:
          - name: qwen3-embedding-8b
            enabled: true
            pvc:
              storage: "20Gi"
            args:
              - llama-server
              - --no-mmap
              - -ngl
              - "999"
              - -fa
              - "on"
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -ub
              - "2048"
              - -b
              - "2048"
              - -c
              - "0"
              - -v
              - --pooling
              - last
              - -hf
              - Qwen/Qwen3-Embedding-8B-GGUF
            resources:
              limits:
                amd.com/gpu: 1
                memory: "25Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "1"
                memory: "15Gi"
            zeroscaling:
              cooldownPeriod: 300
          - name: nomic-embed-code
            enabled: true
            pvc:
              storage: "35Gi"
            args:
              - llama-server
              - --no-mmap
              - -ngl
              - "999"
              - -fa
              - "on"
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -ub
              - "2048"
              - -b
              - "2048"
              - -c
              - "0"
              - -v
              - --pooling
              - last
              - -hf
              - nomic-ai/nomic-embed-code-GGUF
            resources:
              limits:
                amd.com/gpu: 1
                memory: "50Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "1"
                memory: "35Gi"
            zeroscaling:
              cooldownPeriod: 300
          - name: e5-mistral-7b-instruct-embed
            enabled: true
            pvc:
              storage: "20Gi"
            args:
              - llama-server
              - --no-mmap
              - -ngl
              - "999"
              - -fa
              - "on"
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -ub
              - "2048"
              - -b
              - "2048"
              - -c
              - "0"
              - -v
              - -hf
              - second-state/E5-Mistral-7B-Instruct-Embedding-GGUF
            resources:
              limits:
                amd.com/gpu: 1
                memory: "30Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "1"
                memory: "20Gi"
            zeroscaling:
              cooldownPeriod: 300
          - name: gpt-oss-120b
            enabled: true
            pvc:
              storage: "80Gi"
            args:
              - llama-server
              - --no-mmap
              - -ngl
              - "999"
              - -fa
              - "on"
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -ub
              - "2048"
              - -b
              - "2048"
              - -c
              - "0"
              - -v
              - -hf
              - ggml-org/gpt-oss-120b-GGUF
            resources:
              limits:
                amd.com/gpu: 1
                memory: "100Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "1"
                memory: "70Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: gpt-oss-20b
            enabled: true
            pvc:
              storage: "20Gi"
            args:
              - llama-server
              - --no-mmap
              - -ngl
              - "999"
              - -fa
              - "on"
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -ub
              - "2048"
              - -b
              - "2048"
              - -c
              - "0"
              - -v
              - -hf
              - ggml-org/gpt-oss-20b-GGUF
            resources:
              limits:
                amd.com/gpu: 1
                memory: "25Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "1"
                memory: "15Gi"
  ignoreDifferences:
    - group: apps
      kind: Deployment
      namespace: models
      jsonPointers:
        - /spec/replicas
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - RespectIgnoreDifferences=true
    automated:
      prune: true
      selfHeal: true
