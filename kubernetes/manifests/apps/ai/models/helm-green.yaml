apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-models
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "18"
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: models
  source:
    repoURL: https://github.com/blake-hamm/kube-ai-stack.git
    targetRevision: main
    path: charts/kube-ai-stack
    helm:
      valuesObject:
        global:
          prometheusUrl: "http://monitor-prometheus.monitor.svc.cluster.local:9090"
          mlflow:
            enabled: true
          phoenix:
            enabled: true
          litellm:
            manage_config: true
            config:
              litellm_settings:
                timeout: 1800
                stream_timeout: 1800
                request_timeout: 1800
                modify_params: True
                cache: False
                cache_params:
                  type: qdrant-semantic
                  qdrant_semantic_cache_embedding_model: qwen-embed
                  qdrant_collection_name: litellm
                  similarity_threshold: 0.6
                callbacks: ["arize_phoenix"]
                success_callback: ["mlflow"]
                failure_callback: ["mlflow"]
              general_settings:
                max_parallel_requests: 5
                global_max_parallel_requests: 5
                store_model_in_db: true
                store_prompts_in_spend_logs: true
                health_check_interval: 1800
              router_settings:
                debug_level: "INFO"
                cooldown_time: 30
                allowed_fails: 999
                timeout: 1800
                num_retries: 1
                retry_after: 15
                routing_strategy: simple-shuffle
                redis_host: os.environ/REDIS_HOST
                redis_port: os.environ/REDIS_PORT
                redis_password: os.environ/REDIS_PASSWORD
        mlflow:
          backendStore:
            databaseMigration: true
            databaseConnectionCheck: true
            postgres:
              enabled: true
              host: "models-postgresql-rw"
              port: 5432
              database: "mlflow"
            existingDatabaseSecret:
              name: "models-postgresql-app"
              usernameKey: "username"
              passwordKey: "password"
          artifactRoot:
            s3:
              enabled: true
              bucket: mlflow
              existingSecret:
                name: "models-external-secret"
                keyOfAccessKeyId: "access_key_id"
                keyOfSecretAccessKey: "secret_access_key"
          extraEnvVars:
            MLFLOW_S3_ENDPOINT_URL: http://seaweedfs-s3.seaweedfs.svc.cluster.local:8333
            MLFLOW_S3_IGNORE_TLS: "true"
            MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE: "true"
          serviceMonitor:
            enabled: true
            namespace: models
        phoenix-helm:
          ingress:
            enabled: false
          postgresql:
            enabled: false
          persistence:
            enabled: false
          server:
            telemetryEnabled: false
          service:
            type: "ClusterIP"
          auth:
            enableAuth: false
            createSecret: false
          additionalEnv:
            - name: PHOENIX_POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: host
            - name: PHOENIX_POSTGRES_PORT
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: port
            - name: PHOENIX_POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: user
            - name: PHOENIX_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: password
            - name: PHOENIX_POSTGRES_DB
              value: "phoenix"
        models:
          - name: qwen-embed
            description: Great all around embedding model
            enabled: true
            pvc:
              storage: "30Gi"
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: server-rocm-b7850
            args:
              - -hf
              - Qwen/Qwen3-Embedding-8B-GGUF:F16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "512"
              - -c
              - "8192"
              - --parallel
              - "4"
              - --cont-batching
            resources:
              limits:
                amd.com/gpu: 1
                memory: "26Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "20Gi"
            nodeSelector:
              kubernetes.io/hostname: green-talos-worker-amd-gpu
            hfToken:
              enabled: true
              secretName: models-external-secret
              secretKey: hf_token
          - name: gpt-oss-120b
            description: Great reasoning MoE model for general chat queries
            enabled: true
            pvc:
              storage: "80Gi"
            image:
              tag: rocm7-nightlies_20260205T023611
            args:
              - llama-server
              - -hf
              - gghfez/gpt-oss-120b-Derestricted.MXFP4_MOE-gguf
              # - bartowski/openai_gpt-oss-120b-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "1.0"
              - --min-p
              - "0.0"
              - --top-p
              - "1.0"
              - --top-k
              - "0.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen
            description: High quality dense model
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Qwen3-32B-GGUF:F16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.00"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-instruct
            description: High quality MoE model for general chat
            enabled: true
            pvc:
              storage: "40Gi"
            image:
              tag: vulkan-amdvlk_20251128T163204
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF
              - -hff
              - UD-Q6_K_XL/Qwen3-Next-80B-A3B-Instruct-UD-Q6_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.00"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-thinking
            description: High quality, reasoning MoE model for general chat
            enabled: true
            pvc:
              storage: "70Gi"
            image:
              tag: vulkan-amdvlk
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF
              - -hff
              - UD-Q6_K_XL/Qwen3-Next-80B-A3B-Thinking-UD-Q6_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.6"
              - --min-p
              - "0.00"
              - --top-p
              - "0.95"
              - --top-k
              - "20"
              - --presence-penalty
              - "1"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-coder
            description: High quality MoE model for coding
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.0"
              - --top-p
              - "0.80"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: devstral
            description: Mistral development model
            enabled: true
            pvc:
              storage: "80Gi"
            image:
              tag: vulkan-radv_20260211T025037
            args:
              - llama-server
              - -hf
              - unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:BF16
              # - unsloth/Devstral-2-123B-Instruct-2512-GGUF
              # - -hff
              # - UD-Q4_K_XL/Devstral-2-123B-Instruct-2512-UD-Q4_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "1024"
              - -b
              - "1024"
              - -c
              - "65536"
              - -ctk
              - q8_0
              - -ctv
              - q8_0
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.15"
              - --min-p
              - "0.01"
            template: |
              {#- Unsloth template fixes #}
              {%- set yesterday_day = strftime_now("%d") %}
              {%- set yesterday_month = strftime_now("%m") %}
              {%- set yesterday_year = strftime_now("%Y") %}
              {%- set today_date = yesterday_year + '-' + yesterday_month + '-' + yesterday_day %}
              {%- if yesterday_day == '01' %}
                  {#- Jinja doesnt allow minus 1 date - Unsloth alternative #}
                  {%- if yesterday_month == '01' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '12' %}
                      {%- if yesterday_year == '2024' %}
                          {%- set yesterday_year = '2023' %}
                      {%- elif yesterday_year == '2025' %}
                          {%- set yesterday_year = '2024' %}
                      {%- elif yesterday_year == '2026' %}
                          {%- set yesterday_year = '2025' %}
                      {%- elif yesterday_year == '2027' %}
                          {%- set yesterday_year = '2026' %}
                      {%- elif yesterday_year == '2028' %}
                          {%- set yesterday_year = '2027' %}
                      {%- elif yesterday_year == '2029' %}
                          {%- set yesterday_year = '2028' %}
                      {%- elif yesterday_year == '2030' %}
                          {%- set yesterday_year = '2029' %}
                      {%- elif yesterday_year == '2031' %}
                          {%- set yesterday_year = '2030' %}
                      {%- elif yesterday_year == '2032' %}
                          {%- set yesterday_year = '2031' %}
                      {%- elif yesterday_year == '1970' %}
                          {#- Stop llama_cpp from erroring out #}
                          {%- set yesterday_year = '1970' %}
                      {%- else %}
                          {{- raise_exception('Unsloth custom template does not support years > 2032. Error year = [' + yesterday_year + ']') }}
                      {%- endif %}
                  {%- elif yesterday_month == '02' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '01' %}
                  {%- elif yesterday_month == '03' %}
                      {%- set yesterday_month = '02' %}
                      {%- set yesterday_day = '28' %}
                      {%- if yesterday_year == '2024' %}
                          {%- set yesterday_day = '29' %}
                      {%- elif yesterday_year == '2028' %}
                          {%- set yesterday_day = '29' %}
                      {%- elif yesterday_year == '2032' %}
                          {%- set yesterday_day = '29' %}
                      {%- elif yesterday_year == '1970' %}
                          {#- Stop llama_cpp from erroring out #}
                          {%- set yesterday_day = '29' %}
                      {%- else %}
                          {{- raise_exception('Unsloth custom template does not support years > 2032. Error year = [' + yesterday_year + ']') }}
                      {%- endif %}
                  {%- elif yesterday_month == '04' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '03' %}
                  {%- elif yesterday_month == '05' %}
                      {%- set yesterday_day = '30' %}
                      {%- set yesterday_month = '04' %}
                  {%- elif yesterday_month == '06' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '05' %}
                  {%- elif yesterday_month == '07' %}
                      {%- set yesterday_day = '30' %}
                      {%- set yesterday_month = '06' %}
                  {%- elif yesterday_month == '08' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '07' %}
                  {%- elif yesterday_month == '09' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '08' %}
                  {%- elif yesterday_month == '10' %}
                      {%- set yesterday_day = '30' %}
                      {%- set yesterday_month = '09' %}
                  {%- elif yesterday_month == '11' %}
                      {%- set yesterday_day = '31' %}
                      {%- set yesterday_month = '10' %}
                  {%- elif yesterday_month == '12' %}
                      {%- set yesterday_day = '30' %}
                      {%- set yesterday_month = '11' %}
                  {%- endif %}
              {%- elif yesterday_day == '02' %}
                  {%- set yesterday_day = '01' %}
              {%- elif yesterday_day == '03' %}
                  {%- set yesterday_day = '02' %}
              {%- elif yesterday_day == '04' %}
                  {%- set yesterday_day = '03' %}
              {%- elif yesterday_day == '05' %}
                  {%- set yesterday_day = '04' %}
              {%- elif yesterday_day == '06' %}
                  {%- set yesterday_day = '05' %}
              {%- elif yesterday_day == '07' %}
                  {%- set yesterday_day = '06' %}
              {%- elif yesterday_day == '08' %}
                  {%- set yesterday_day = '07' %}
              {%- elif yesterday_day == '09' %}
                  {%- set yesterday_day = '08' %}
              {%- elif yesterday_day == '10' %}
                  {%- set yesterday_day = '09' %}
              {%- elif yesterday_day == '11' %}
                  {%- set yesterday_day = '10' %}
              {%- elif yesterday_day == '12' %}
                  {%- set yesterday_day = '11' %}
              {%- elif yesterday_day == '13' %}
                  {%- set yesterday_day = '12' %}
              {%- elif yesterday_day == '14' %}
                  {%- set yesterday_day = '13' %}
              {%- elif yesterday_day == '15' %}
                  {%- set yesterday_day = '14' %}
              {%- elif yesterday_day == '16' %}
                  {%- set yesterday_day = '15' %}
              {%- elif yesterday_day == '17' %}
                  {%- set yesterday_day = '16' %}
              {%- elif yesterday_day == '18' %}
                  {%- set yesterday_day = '17' %}
              {%- elif yesterday_day == '19' %}
                  {%- set yesterday_day = '18' %}
              {%- elif yesterday_day == '20' %}
                  {%- set yesterday_day = '19' %}
              {%- elif yesterday_day == '21' %}
                  {%- set yesterday_day = '20' %}
              {%- elif yesterday_day == '22' %}
                  {%- set yesterday_day = '21' %}
              {%- elif yesterday_day == '23' %}
                  {%- set yesterday_day = '22' %}
              {%- elif yesterday_day == '24' %}
                  {%- set yesterday_day = '23' %}
              {%- elif yesterday_day == '25' %}
                  {%- set yesterday_day = '24' %}
              {%- elif yesterday_day == '26' %}
                  {%- set yesterday_day = '25' %}
              {%- elif yesterday_day == '27' %}
                  {%- set yesterday_day = '26' %}
              {%- elif yesterday_day == '28' %}
                  {%- set yesterday_day = '27' %}
              {%- elif yesterday_day == '29' %}
                  {%- set yesterday_day = '28' %}
              {%- elif yesterday_day == '30' %}
                  {%- set yesterday_day = '29' %}
              {%- elif yesterday_day == '31' %}
                  {%- set yesterday_day = '30' %}
              {%- endif %}
              {#- Edits made by Unsloth #}
              {%- set yesterday_date = yesterday_year + '-' + yesterday_month + '-' + yesterday_day %}
              {#- Default system message if no system prompt is passed. #}
              {%- set default_system_message = "You are Devstral-Small-2-24B-Instruct-2512, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYou power an AI assistant called Le Chat.\nYour knowledge base was last updated on 2023-10-01.\nThe current date is " + today_date + ".\n\nWhen you\'re not sure about some information or when the user\'s request requires up-to-date or specific data, you must use the available tools to fetch the information. Do not hesitate to use tools whenever they can provide a more accurate or complete response. If no relevant tools are available, then clearly state that you don\'t have the information and avoid making up anything.\nIf the user\'s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\").\nYou are always very attentive to dates, in particular you try to resolve dates (e.g. \"yesterday\" is " + yesterday_date + ") and when asked about information at specific dates, you discard information that is at another date.\nYou follow these instructions in all languages, and always respond to the user in the language they use or request.\nNext sections describe the capabilities that you have.\n\n# WEB BROWSING INSTRUCTIONS\n\nYou cannot perform any web search or access internet to open URLs, links etc. If it seems like the user is expecting you to do so, you clarify the situation and ask the user to copy paste the text directly in the chat.\n\n# MULTI-MODAL INSTRUCTIONS\n\nYou have the ability to read images, but you cannot generate images. You also cannot transcribe audio files or videos.\nYou cannot read nor transcribe audio files or videos.\n\n# TOOL CALLING INSTRUCTIONS\n\nYou may have access to tools that you can use to fetch information or perform actions. You must use these tools in the following situations:\n\n1. When the request requires up-to-date information.\n2. When the request requires specific data that you do not have in your knowledge base.\n3. When the request involves actions that you cannot perform without tools.\n\nAlways prioritize using tools to provide the most accurate and helpful response. If tools are not available, inform the user that you cannot perform the requested action at the moment." %}

              {#- Begin of sequence token. #}
              {{- bos_token }}

              {#- Handle system prompt if it exists. #}
              {#- System prompt supports text content or text chunks. #}
              {%- if messages[0]['role'] == 'system' %}
                  {{- '[SYSTEM_PROMPT]' -}}
                  {%- if messages[0]['content'] is string %}
                      {{- messages[0]['content'] -}}
                  {%- else %}
                      {%- for block in messages[0]['content'] %}
                          {%- if block['type'] == 'text' %}
                              {{- block['text'] }}
                          {%- else %}
                              {{- raise_exception('Only text chunks are supported in system message contents.') }}
                          {%- endif %}
                      {%- endfor %}
                  {%- endif %}
                  {{- '[/SYSTEM_PROMPT]' -}}
                  {%- set loop_messages = messages[1:] %}
              {%- else %}
                  {%- set loop_messages = messages %}
                  {%- if default_system_message != '' %}
                      {{- '[SYSTEM_PROMPT]' + default_system_message + '[/SYSTEM_PROMPT]' }}
                  {%- endif %}
              {%- endif %}


              {#- Tools definition #}
              {%- set tools_definition = '' %}
              {%- set has_tools = false %}
              {%- if tools is defined and tools is not none and tools|length > 0 %}
                  {%- set has_tools = true %}
                  {%- set tools_definition = '[AVAILABLE_TOOLS]' + (tools| tojson) + '[/AVAILABLE_TOOLS]' %}
                  {{- tools_definition }}
              {%- endif %}

              {#- Checks for alternating user/assistant messages. #}
              {%- set ns = namespace(index=0) %}
              {%- for message in loop_messages %}
                  {%- if message.role == 'user' or (message.role == 'assistant' and (message.tool_calls is not defined or message.tool_calls is none or message.tool_calls | length == 0)) %}
                      {%- set ns.index = ns.index + 1 %}
                  {%- endif %}
              {%- endfor %}

              {#- Handle conversation messages. #}
              {%- for message in loop_messages %}

                  {#- User messages supports text content or text and image chunks. #}
                  {%- if message['role'] == 'user' %}
                      {%- if message['content'] is string %}
                          {{- '[INST]' + message['content'] + '[/INST]' }}
                      {%- elif message['content'] | length > 0 %}
                          {{- '[INST]' }}
                          {%- if message['content'] | length == 2 %}
                              {%- set blocks = message['content'] | sort(attribute='type') %}
                          {%- else %}
                              {%- set blocks = message['content'] %}
                          {%- endif %}
                          {%- for block in blocks %}
                              {%- if block['type'] == 'text' %}
                                  {{- block['text'] }}
                              {%- elif block['type'] in ['image', 'image_url'] %}
                                  {{- '[IMG]' }}
                              {%- else %}
                                  {{- raise_exception('Only text, image and image_url chunks are supported in user message content.') }}
                              {%- endif %}
                          {%- endfor %}
                          {{- '[/INST]' }}
                      {%- else %}
                          {{- raise_exception('User message must have a string or a list of chunks in content') }}
                      {%- endif %}

                  {#- Assistant messages supports text content or text and image chunks. #}
                  {%- elif message['role'] == 'assistant' %}
                      {%- if (message['content'] is none or message['content'] == '' or message['content']|length == 0) and (message['tool_calls'] is not defined or message['tool_calls'] is none or message['tool_calls']|length == 0) %}
                          {{- raise_exception('Assistant message must have a string or a list of chunks in content or a list of tool calls.') }}
                      {%- endif %}

                      {%- if message['content'] is string %}
                          {{- message['content'] }}
                      {%- elif message['content'] is iterable and message['content'] | length > 0 %}
                          {%- for block in message['content'] %}
                              {%- if block['type'] == 'text' %}
                                  {{- block['text'] }}
                              {%- else %}
                                  {{- raise_exception('Only text chunks are supported in assistant message contents.') }}
                              {%- endif %}
                          {%- endfor %}
                      {%- endif %}

                      {%- if message['tool_calls'] is defined and message['tool_calls'] is not none and message['tool_calls']|length > 0 %}
                          {%- for tool in message['tool_calls'] %}
                              {%- set arguments = tool['function']['arguments'] %}
                              {%- if arguments is not string %}
                                  {%- set arguments = arguments|tojson|safe %}
                              {%- elif arguments == '' %}
                                  {%- set arguments = '{}' %}
                              {%- endif %}
                              {{- '[TOOL_CALLS]' + tool['function']['name'] + '[ARGS]' + arguments }}
                          {%- endfor %}
                      {%- endif %}

                      {#- End of sequence token for each assistant messages. #}
                      {{- eos_token }}

                  {#- Tool messages only supports text content. #}
                  {%- elif message['role'] == 'tool' %}
                      {{- '[TOOL_RESULTS]' + message['content']|string + '[/TOOL_RESULTS]' }}

                  {%- endif %}
              {%- endfor %}
              {#- Copyright 2025-present Unsloth. Apache 2.0 License. #}
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: nemotron
            description: Reasoning model from nvidia
            enabled: true
            image:
              tag: vulkan-radv_20260211T025037
            pvc:
              storage: "70Gi"
            args:
              - llama-server
              - -hf
              - unsloth/Nemotron-3-Nano-30B-A3B-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "1024"
              - -b
              - "1024"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.8"
              - --top-p
              - "0.98"
              - --min_p
              - "0.01"
          - name: gemma-3
            description: Mid model with multimodal support
            enabled: true
            image:
              tag: rocm-6.4.4
            pvc:
              storage: "60Gi"
            args:
              - llama-server
              - -hf
              - unsloth/gemma-3-27b-it-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.0"
              - --repeat-penalty
              - "1.0"
              - --min-p
              - "0.01"
              - --top-k
              - "64"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "100Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "80Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air-reap
            description: High quality MoE model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            image:
              tag: vulkan-amdvlk
            args:
              - llama-server
              - -hf
              - bartowski/cerebras_GLM-4.5-Air-REAP-82B-A12B-GGUF
              - -hff
              - cerebras_GLM-4.5-Air-REAP-82B-A12B-Q4_K_L/cerebras_GLM-4.5-Air-REAP-82B-A12B-Q4_K_L-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air
            description: High quality dense model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - unsloth/GLM-4.5-Air-GGUF
              - -hff
              - UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-flash
            description: High quality moe model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "70Gi"
            image:
              tag: rocm7-nightlies
            args:
              - llama-server
              - -hf
              - unsloth/GLM-4.7-Flash-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.8"
              - --top-p
              - "0.99"
              - --min-p
              - "0.01"
              - --repeat-penalty
              - "1.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: llama-4
            description: High quality multimodal model
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_M
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --min-p
              - "0.01"
              - --top-p
              - "0.9"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: seed-oss
            description: Mid model specialized in agentic and coding
            enabled: true
            pvc:
              storage: "90Gi"
            image:
              tag: rocm7-nightlies
            args:
              - llama-server
              - -hf
              - unsloth/Seed-OSS-36B-Instruct-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.1"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
  ignoreDifferences:
    - group: apps
      kind: Deployment
      namespace: models
      jsonPointers:
        - /spec/replicas
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - RespectIgnoreDifferences=true
    automated:
      prune: true
      selfHeal: true
