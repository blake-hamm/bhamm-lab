apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-models
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "18"
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: models
  source:
    repoURL: https://github.com/blake-hamm/bhamm-lab.git
    targetRevision: feature/expose-lab
    path: kubernetes/charts/llm-models
    helm:
      valuesObject:
        models:
          - name: qwen-embed
            description: Great all around embedding model
            enabled: true
            pvc:
              storage: "20Gi"
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: server-vulkan
            args:
              - -hf
              - Qwen/Qwen3-Embedding-8B-GGUF:F16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "512"
              - -c
              - "8192"
              - --parallel
              - "4"
              - --cont-batching
            resources:
              limits:
                amd.com/gpu: 1
                memory: "26Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "20Gi"
            nodeSelector:
              kubernetes.io/hostname: green-talos-worker-amd-gpu
          - name: gpt-oss-120b
            description: Great reasoning MoE model for general chat queries
            enabled: true
            pvc:
              storage: "80Gi"
            image:
              tag: vulkan-amdvlk
            args:
              - llama-server
              - -hf
              - bartowski/openai_gpt-oss-120b-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "1.0"
              - --min-p
              - "0.0"
              - --top-p
              - "1.0"
              - --top-k
              - "0.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen
            description: High quality dense model
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-VL-32B-Instruct-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --repeat-penalty
              - "1.0"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-instruct
            description: High quality MoE model for general chat
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --repeat-penalty
              - "1.0"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-thinking
            description: High quality, reasoning MoE model for general chat
            enabled: true
            pvc:
              storage: "70Gi"
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-VL-30B-A3B-Thinking-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "1"
              - --min-p
              - "0.0"
              - --top-p
              - "0.95"
              - --top-k
              - "20"
              - --repeat-penalty
              - "1.0"
              - --presence-penalty
              - "0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-coder
            description: High quality MoE model for coding
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ctk
              - q8_0
              - -ctv
              - q8_0
              - -ub
              - "3072"
              - -b
              - "49152"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.0"
              - --top-p
              - "0.80"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: gemma-3
            description: Mid model with multimodal support
            enabled: true
            image:
              tag: rocm-6.4.4-rocwmma
            pvc:
              storage: "60Gi"
            args:
              - llama-server
              - -hf
              - unsloth/gemma-3-27b-it-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.0"
              - --repeat-penalty
              - "1.0"
              - --min-p
              - "0.01"
              - --top-k
              - "64"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "100Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "80Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air-reap
            description: High quality MoE model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            image:
              tag: vulkan-radv
            args:
              - llama-server
              - -hf
              - bartowski/cerebras_GLM-4.5-Air-REAP-82B-A12B-GGUF:Q4_K_L
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air
            description: High quality dense model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - unsloth/GLM-4.5-Air-GGUF:Q4_K_XL
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: llama-4
            description: High quality multimodal model
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_M
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --min-p
              - "0.01"
              - --top-p
              - "0.9"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: seed-oss
            description: Mid model specialized in agentic and coding
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - unsloth/Seed-OSS-36B-Instruct-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.1"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
  ignoreDifferences:
    - group: apps
      kind: Deployment
      namespace: models
      jsonPointers:
        - /spec/replicas
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - RespectIgnoreDifferences=true
    automated:
      prune: true
      selfHeal: true
