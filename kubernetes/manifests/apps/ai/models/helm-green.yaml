apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: llm-models
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "18"
spec:
  project: default
  destination:
    server: https://kubernetes.default.svc
    namespace: models
  source:
    repoURL: https://github.com/blake-hamm/kube-ai-stack.git
    targetRevision: main
    path: charts/kube-ai-stack
    helm:
      valuesObject:
        global:
          prometheusUrl: "http://monitor-prometheus.monitor.svc.cluster.local:9090"
          mlflow:
            enabled: true
          phoenix:
            enabled: true
          litellm:
            manage_config: true
            config:
              litellm_settings:
                timeout: 1800
                stream_timeout: 1800
                request_timeout: 1800
                cache: False
                cache_params:
                  type: qdrant-semantic
                  qdrant_semantic_cache_embedding_model: qwen-embed
                  qdrant_collection_name: litellm
                  similarity_threshold: 0.6
                callbacks: ["arize_phoenix"]
                success_callback: ["mlflow"]
                failure_callback: ["mlflow"]
              general_settings:
                max_parallel_requests: 5
                global_max_parallel_requests: 5
                store_model_in_db: true
                store_prompts_in_spend_logs: true
                health_check_interval: 1800
              router_settings:
                debug_level: "INFO"
                cooldown_time: 30
                allowed_fails: 999
                timeout: 1800
                num_retries: 1
                retry_after: 15
                routing_strategy: simple-shuffle
                redis_host: os.environ/REDIS_HOST
                redis_port: os.environ/REDIS_PORT
                redis_password: os.environ/REDIS_PASSWORD
        mlflow:
          backendStore:
            databaseMigration: true
            databaseConnectionCheck: true
            postgres:
              enabled: true
              host: "models-postgresql-rw"
              port: 5432
              database: "mlflow"
            existingDatabaseSecret:
              name: "models-postgresql-app"
              usernameKey: "username"
              passwordKey: "password"
          artifactRoot:
            s3:
              enabled: true
              bucket: mlflow
              existingSecret:
                name: "models-external-secret"
                keyOfAccessKeyId: "access_key_id"
                keyOfSecretAccessKey: "secret_access_key"
          extraEnvVars:
            MLFLOW_S3_ENDPOINT_URL: http://seaweedfs-s3.seaweedfs.svc.cluster.local:8333
            MLFLOW_S3_IGNORE_TLS: "true"
            MLFLOW_SERVER_DISABLE_SECURITY_MIDDLEWARE: "true"
          serviceMonitor:
            enabled: true
            namespace: models
        phoenix-helm:
          ingress:
            enabled: false
          postgresql:
            enabled: false
          persistence:
            enabled: false
          server:
            telemetryEnabled: false
          service:
            type: "ClusterIP"
          auth:
            enableAuth: false
            createSecret: false
          additionalEnv:
            - name: PHOENIX_POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: host
            - name: PHOENIX_POSTGRES_PORT
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: port
            - name: PHOENIX_POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: user
            - name: PHOENIX_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: models-postgresql-app
                  key: password
            - name: PHOENIX_POSTGRES_DB
              value: "phoenix"
        models:
          - name: qwen-embed
            description: Great all around embedding model
            enabled: true
            pvc:
              storage: "30Gi"
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: server-rocm-b7850
            args:
              - -hf
              - Qwen/Qwen3-Embedding-8B-GGUF:F16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --embeddings
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "512"
              - -c
              - "8192"
              - --parallel
              - "4"
              - --cont-batching
            resources:
              limits:
                amd.com/gpu: 1
                memory: "26Gi"
                cpu: "4"
              requests:
                amd.com/gpu: 1
                cpu: "2"
                memory: "20Gi"
            nodeSelector:
              kubernetes.io/hostname: green-talos-worker-amd-gpu
            hfToken:
              enabled: true
              secretName: models-external-secret
              secretKey: hf_token
          - name: gpt-oss-120b
            description: Great reasoning MoE model for general chat queries
            enabled: true
            pvc:
              storage: "80Gi"
            image:
              tag: rocm7-nightlies_20260205T023611
            args:
              - llama-server
              - -hf
              - gghfez/gpt-oss-120b-Derestricted.MXFP4_MOE-gguf
              # - bartowski/openai_gpt-oss-120b-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "1.0"
              - --min-p
              - "0.0"
              - --top-p
              - "1.0"
              - --top-k
              - "0.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen
            description: High quality dense model
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Qwen3-32B-GGUF:F16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.00"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-instruct
            description: High quality MoE model for general chat
            enabled: true
            pvc:
              storage: "40Gi"
            image:
              tag: vulkan-amdvlk_20251128T163204
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF
              - -hff
              - UD-Q6_K_XL/Qwen3-Next-80B-A3B-Instruct-UD-Q6_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.00"
              - --top-p
              - "0.8"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.5"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-thinking
            description: High quality, reasoning MoE model for general chat
            enabled: true
            pvc:
              storage: "70Gi"
            image:
              tag: vulkan-amdvlk
            args:
              - llama-server
              - -hf
              - unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF
              - -hff
              - UD-Q6_K_XL/Qwen3-Next-80B-A3B-Thinking-UD-Q6_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "2"
              - --cont-batching
              - --temp
              - "0.6"
              - --min-p
              - "0.00"
              - --top-p
              - "0.95"
              - --top-k
              - "20"
              - --presence-penalty
              - "1"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: qwen-coder
            description: High quality MoE model for coding
            enabled: true
            pvc:
              storage: "40Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.7"
              - --min-p
              - "0.0"
              - --top-p
              - "0.80"
              - --top-k
              - "20"
              - --presence-penalty
              - "1.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: devstral
            description: Mistral development model
            enabled: true
            pvc:
              storage: "80Gi"
            image:
              tag: vulkan-radv_20260211T025037
            args:
              - llama-server
              - -hf
              - unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF:BF16
              # - unsloth/Devstral-2-123B-Instruct-2512-GGUF
              # - -hff
              # - UD-Q4_K_XL/Devstral-2-123B-Instruct-2512-UD-Q4_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "1024"
              - -b
              - "1024"
              - -c
              - "65536"
              - -ctk
              - q8_0
              - -ctv
              - q8_0
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.15"
              - --min-p
              - "0.01"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: gemma-3
            description: Mid model with multimodal support
            enabled: true
            image:
              tag: rocm-6.4.4
            pvc:
              storage: "60Gi"
            args:
              - llama-server
              - -hf
              - unsloth/gemma-3-27b-it-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.0"
              - --repeat-penalty
              - "1.0"
              - --min-p
              - "0.01"
              - --top-k
              - "64"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "100Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "80Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air-reap
            description: High quality MoE model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            image:
              tag: vulkan-amdvlk
            args:
              - llama-server
              - -hf
              - bartowski/cerebras_GLM-4.5-Air-REAP-82B-A12B-GGUF
              - -hff
              - cerebras_GLM-4.5-Air-REAP-82B-A12B-Q4_K_L/cerebras_GLM-4.5-Air-REAP-82B-A12B-Q4_K_L-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-air
            description: High quality dense model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - unsloth/GLM-4.5-Air-GGUF
              - -hff
              - UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --top-p
              - "0.95"
              - --top-k
              - "40"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: glm-flash
            description: High quality moe model for agents, coding or complex chat
            enabled: true
            pvc:
              storage: "70Gi"
            image:
              tag: rocm7-nightlies
            args:
              - llama-server
              - -hf
              - unsloth/GLM-4.7-Flash-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.8"
              - --top-p
              - "0.99"
              - --min-p
              - "0.01"
              - --repeat-penalty
              - "1.0"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "120Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "110Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: llama-4
            description: High quality multimodal model
            enabled: true
            pvc:
              storage: "90Gi"
            args:
              - llama-server
              - -hf
              - ggml-org/Llama-4-Scout-17B-16E-Instruct-GGUF:Q4_K_M
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "0.6"
              - --min-p
              - "0.01"
              - --top-p
              - "0.9"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
          - name: seed-oss
            description: Mid model specialized in agentic and coding
            enabled: true
            pvc:
              storage: "90Gi"
            image:
              tag: rocm7-nightlies
            args:
              - llama-server
              - -hf
              - unsloth/Seed-OSS-36B-Instruct-GGUF:BF16
              - --host
              - 0.0.0.0
              - --metrics
              - --no-webui
              - --jinja
              - --timeout
              - "1800"
              - -v
              - --no-mmap
              - --no-warmup
              - -ngl
              - "999"
              - -fa
              - "on"
              - -ub
              - "512"
              - -b
              - "4096"
              - -c
              - "65536"
              - --parallel
              - "1"
              - --cont-batching
              - --temp
              - "1.1"
              - --top-p
              - "0.95"
            resources:
              limits:
                amd.com/gpu: 1
                memory: "110Gi"
                cpu: "8"
              requests:
                amd.com/gpu: 1
                cpu: "4"
                memory: "90Gi"
            zeroscaling:
              cooldownPeriod: 1800
  ignoreDifferences:
    - group: apps
      kind: Deployment
      namespace: models
      jsonPointers:
        - /spec/replicas
  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ApplyOutOfSyncOnly=true
      - RespectIgnoreDifferences=true
    automated:
      prune: true
      selfHeal: true
